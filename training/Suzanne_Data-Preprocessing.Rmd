---
title: "AI Project - Data Preprocessing"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


First, we prepare the working environment.
```{r}
rm(list=ls()) # Clear the environment
# setwd("...")  # Setup the working directory
```

# Import libraries
```{r, message=FALSE}
if(!require(textclean)){
  install.packages("textclean")
  library(textclean)
}

if(!require(hunspell)){
  install.packages("hunspell")
  library(hunspell)
}

if(!require(e1071)){
  install.packages("e1071")
  library(e1071)
}


# Update textclean to latest version
#install.packages("remotes")
#remotes::install_github("trinker/textclean")

library(tm)
library(SnowballC)
library(textclean)     # for cleaning the text data
library(hunspell)      # for checking spelling
library(stringr)
library(wordcloud)
library(ggplot2)
library(caTools)
library(dplyr)
library(lexicon)       
```

# Data Preparation

Load the data.
```{r}
twitter <- read.csv("twitterdata.csv", stringsAsFactors = FALSE)
twitter <- twitter[,-1] # remove the (first) keyword column
twitter <- subset(twitter, select=c(2,1))  # reorder columns so the tweet comes first, then the label

str(twitter) # Internal structure of the dataframe # sometimes data will be of string type
summary(twitter) # Summary of the data
head(twitter)

table(twitter$label)  # counts for each label

twitter
```

## Data Cleaning

### Using the textclean library

*   `replace_emoticon()` converts emoticons to words with equivalent meaning
*   `replace_contraction()` converts internet slang to longer word equivalents
*   `replace_internet_slang()` converts internet slang to longer word equivalents

```{r}
library(hunspell)
library(textclean)

# Test the new functions
replace_emoticon("hello :)")
replace_emoticon("hello:)")
replace_contraction("it isn't")
replace_internet_slang("TGIF")
replace_word_elongation("amazinggg")
str_replace_all("&lt;&lt;3", "&lt;", "<") # we convert it to the symbol as the symbol is sometimes used in emoticons like the heart <3

```

Notice that the replace_emoticon() function is context-sensitive and only works in certain cases, as such, we need an improved function. We use the function below, sourced from: https://stackoverflow.com/questions/62270337/replace-emoticon-function-incorrectly-replaces-characters-within-a-word-r
```{r}
# fix for some cases of the replace_emoticon() function

replace_emoticon_new <- function (x, emoticon_dt = lexicon::hash_emoticons, ...) 
{
  regex_escape <- function(string) {
    gsub("([][{}()+*^${|\\\\?.])", "\\\\\\1", string)
  }

  stringi::stri_replace_all(x, 
                            regex = paste0("\\s+", regex_escape(emoticon_dt[["x"]])),
                            replacement = paste0(" ", emoticon_dt[['y']]),   
                            vectorize_all = FALSE)
}

replace_emoticon_new("hello :)")
```


```{r}
# Test sequence of functions for one tweet
trystr <- "&lt;3 Gud night on south street. But now i'm feeln lonely &amp; sad ona rainy day :( amazingggg&gt;"
trystr <- str_replace_all(trystr, "&gt;", ">")          # Replace unicode
trystr <- str_replace_all(trystr, "&lt;", "<")
trystr <- str_replace_all(trystr, "&amp;", "and")
trystr <- replace_emoticon(trystr)                      # Replace emoticon
trystr <- replace_emoticon_new(trystr)                  # Replace emoticon (for other cases)
trystr <- replace_contraction(trystr)                   # Replace contractions
trystr <- replace_internet_slang(trystr)                # Replace internet slang
trystr <- replace_word_elongation(trystr)
trystr
```

### Removing URLs
```{r}
trystr2 <- "I met the inspirational patent attorney and Parkinson's campaigner Bryn Williams @wobblywilliams for today's Herald https://t.co/PvobLKri0t testing if text is after"
trystr2 <- gsub("(s?)(f|ht)tp(s?)://\\S+\\b", "", trystr2)
trystr2
```



```{r}
library(hunspell)
library(textclean)
library(stringr)

twitter_clean <- twitter

for (i in 1:length(twitter_clean$tweet)){
  convstr <- twitter_clean[i,1]
  #print(convstr)
  convstr <- gsub("(s?)(f|ht)tp(s?)://\\S+\\b", "", convstr)      # Remove URL links
  convstr <- str_replace_all(convstr, "&gt;", ">")          # Replace unicode
  convstr <- str_replace_all(convstr, "&lt;", "<")
  convstr <- str_replace_all(convstr, "&amp;", "and")
  convstr <- replace_emoticon(convstr)                      # Replace emoticon
  convstr <- replace_emoticon_new(convstr)                  # Replace emoticon (for other cases)
  convstr <- replace_contraction(convstr)                   # Replace contractions
  convstr <- replace_internet_slang(convstr)                # Replace internet slang
  convstr <- replace_word_elongation(convstr)               # Replace elongated words
  twitter_clean[i,1] <- convstr
  }

#twitter_clean

twitter[1806,1]           # original text
twitter_clean[1806,1]     # processed texts

twitter[2103,1]           # original text
twitter_clean[2103,1]     # processed texts
```

From this test example, &amp; was successfully converted to "and" and the :) was converted to "smiley", however, the :( was not converted. When the string "pain:(" is tested (as shown below), the function has no problem converting it into "pain frown", so we are not sure why it is not converted similarly in the dataset. Such inconsistencies could contribute to lowered accuracy later on.
```{r}
replace_emoticon("pain:(")
```

From the second example, we also see that the link was successfully removed.


Let's check that the tweet contents that appear after a URL link are still present.
```{r}
twitter[1882,1]           # original text
twitter_clean[1882,1]     # processed texts

twitter[2858,1]           # original text
twitter_clean[2858,1]     # processed texts
```


# Conduct a train-test split
```{r}
library(caTools)
set.seed(555) 
sample = sample.split(twitter_clean$tweet, SplitRatio = .70)
train = subset(twitter_clean, sample == TRUE)
test  = subset(twitter_clean, sample == FALSE)
```


# Save the processed data

We pre-save the data so it is easier for multiple group members to work with the processed data when trying out models.
```{r}

write.csv(train,"train_textcleaned.csv")
write.csv(test,"test_textcleaned.csv")

```