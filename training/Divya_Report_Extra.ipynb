{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Divya_Report_Extra.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyP7jwi5eaQNhRH751My4+6H",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dibsondivya/ai-health-event/blob/main/training/Divya_Report_Extra.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Data"
      ],
      "metadata": {
        "id": "_ASxRjMUjFrf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ByKe-74NDtvz",
        "outputId": "f69868cc-c61e-45b7-84f3-5e252ba23b0d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1361\n",
            "3175\n",
            "4536\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "df = pd.read_csv('test_textcleaned.csv')\n",
        "print(len(df))\n",
        "df_new = pd.read_csv('train_textcleaned.csv')\n",
        "print(len(df_new))\n",
        "dftotal = df.append(df_new, ignore_index=True)\n",
        "print(len(dftotal))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plot label counts"
      ],
      "metadata": {
        "id": "0mPPv66VjIId"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dftotal['label'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5qdjQx70Fviz",
        "outputId": "7b94617a-b68b-424c-cd5b-a0cb32d7b665"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    2456\n",
              "0    1268\n",
              "2     512\n",
              "3     300\n",
              "Name: label, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn\n",
        "seaborn.countplot(x='label', data=dftotal)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "moFdWSzaFDmA",
        "outputId": "a3bc0857-e533-45cb-f5bf-14bdd4e2fb62"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7ff1ebd3fd90>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dftotal['label'].value_counts().plot(kind = 'barh')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "E-TxgBbwEbnT",
        "outputId": "dfdd73ff-bcd2-42b2-9e14-e516c20facdb"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7ff1eb7555d0>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Classification Report"
      ],
      "metadata": {
        "id": "CZjKjp1drx92"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_true = df['label'].tolist()\n",
        "print(y_true)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ty8TBGNlosGh",
        "outputId": "2c606425-789e-4d19-91cc-f11278adb734"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 2, 1, 1, 1, 1, 1, 3, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 0, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 0, 2, 1, 2, 2, 1, 1, 2, 1, 2, 1, 1, 0, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 2, 2, 1, 2, 2, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 2, 1, 0, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 0, 0, 2, 1, 1, 0, 1, 0, 2, 0, 1, 0, 0, 0, 0, 3, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 0, 0, 0, 2, 0, 1, 0, 0, 0, 1, 0, 0, 1, 3, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 2, 1, 0, 0, 0, 1, 0, 0, 2, 0, 2, 1, 1, 0, 0, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 2, 2, 0, 2, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 2, 0, 1, 0, 0, 0, 1, 3, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 3, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 3, 1, 1, 1, 0, 1, 2, 2, 1, 1, 1, 0, 1, 2, 1, 1, 3, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 0, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 3, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 2, 1, 0, 1, 1, 1, 0, 1, 2, 1, 0, 0, 1, 2, 1, 1, 1, 0, 1, 0, 2, 1, 1, 0, 0, 1, 3, 1, 1, 1, 2, 1, 0, 1, 1, 1, 0, 2, 0, 0, 1, 1, 1, 0, 1, 2, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 3, 1, 0, 1, 1, 0, 3, 1, 0, 1, 2, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 0, 0, 1, 0, 1, 1, 2, 2, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 2, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 2, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 2, 1, 1, 1, 2, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 2, 1, 1, 1, 0, 2, 1, 1, 1, 1, 2, 1, 2, 0, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 2, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 0, 1, 3, 1, 1, 2, 2, 1, 1, 1, 1, 2, 1, 0, 2, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 2, 1, 1, 0, 1, 1, 1, 1, 1, 2, 2, 0, 1, 1, 1, 0, 1, 1, 2, 3, 2, 1, 1, 1, 1, 0, 1, 2, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 2, 1, 1, 1, 1, 3, 0, 2, 0, 2, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 2, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 2, 3, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 0, 1, 1, 1, 2, 0, 3, 3, 3, 3, 1, 3, 1, 3, 0, 1, 1, 3, 3, 1, 3, 0, 1, 3, 0, 1, 1, 3, 1, 1, 3, 3, 1, 1, 1, 1, 1, 1, 0, 3, 1, 1, 1, 2, 1, 3, 1, 3, 3, 1, 1, 3, 1, 0, 3, 1, 3, 0, 3, 1, 1, 1, 3, 3, 3, 3, 3, 2, 3, 1, 1, 1, 1, 3, 3, 1, 2, 1, 0, 1, 3, 3, 1, 2, 3, 3, 3, 1, 3, 1, 1, 3, 1, 2, 1, 1, 3, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 3, 3, 3, 2, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 3, 1, 1, 1, 1, 0, 1, 1, 3, 0, 3, 1, 3, 1, 1, 0, 3, 0, 1, 1, 3, 3, 1, 3, 1, 1, 3, 1, 1, 1, 2, 1, 3, 1, 3, 1, 3, 1, 0, 3, 1, 0, 1, 0, 3, 1, 0, 0, 2, 1, 3, 1, 1, 1, 3, 1, 1, 1, 3, 1, 1, 1, 0, 1, 2, 3, 1, 3, 1, 1, 1, 3, 0, 0, 1, 0, 1, 3, 0, 0, 1, 3, 1, 2, 0, 2, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 2, 2, 0, 0, 0, 0, 0, 2, 2, 1, 0, 2, 0, 0, 0, 1, 2, 0, 2, 2, 2, 0, 0, 0, 2, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 2, 1, 0, 0, 1, 0, 3, 1, 0, 0, 0, 0, 0, 2, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 3, 0, 1, 0, 1, 1, 0, 1, 2, 2, 2, 0, 1, 1, 0, 2, 0, 0, 0, 2, 0, 1, 1, 0, 0, 2, 0, 0, 2, 0, 0, 2, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 2, 0, 0, 1, 0, 3, 0, 3, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 2, 2, 2, 1, 2, 1, 0, 0, 0, 2, 3, 0, 1, 2, 2, 1, 0, 0, 1, 0, 0, 2, 1, 0, 3, 0, 0, 1, 1, 0, 0, 1, 2, 0, 0, 0, 2, 0, 0, 0, 1, 1, 0, 0, 0, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "y_true = df['label'].tolist()\n",
        "y_pred = []\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"dibsondivya/ernie-phmtweets-sutd\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"dibsondivya/ernie-phmtweets-sutd\")\n",
        "\n",
        "tweetlist = df['tweet'].tolist()\n",
        "\n",
        "# Tweet \n",
        "for tweet in tweetlist:\n",
        "  print('tweet is ', tweet)\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "  inputs = tokenizer(tweet, padding=True, truncation=True, return_tensors=\"pt\").to(device) # Move the tensor to the GPU\n",
        "  outputs = model(**inputs)\n",
        "  predictions = torch.argmax(outputs.logits, dim=-1)\n",
        "  print('prediction is ', predictions.item())\n",
        "  y_pred.append(predictions.item())\n",
        "\n",
        "print(y_pred)"
      ],
      "metadata": {
        "id": "gQqsv4HgoLIb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(y_true, y_pred, target_names=['0','1','2','3']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZQLZBYcen0pW",
        "outputId": "76977c00-4fef-49cf-857f-b6c696237c3c"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.85      0.89       392\n",
            "           1       0.93      0.92      0.92       737\n",
            "           2       0.74      0.85      0.79       146\n",
            "           3       0.66      0.85      0.74        86\n",
            "\n",
            "    accuracy                           0.88      1361\n",
            "   macro avg       0.82      0.87      0.84      1361\n",
            "weighted avg       0.89      0.88      0.89      1361\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Failed Attempt to get model summary"
      ],
      "metadata": {
        "id": "wX4ywfcZjLrY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers --quiet\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"dibsondivya/ernie-phmtweets-sutd\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"dibsondivya/ernie-phmtweets-sutd\")\n",
        "import torch\n",
        "from torchsummary import summary"
      ],
      "metadata": {
        "id": "-qXyUK2MI1fV"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fFSzqwfVL8hI",
        "outputId": "6d5d4755-4595-44ee-dbc7-b167846fb112"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(4, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=4, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.num_parameters()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zZiNVD3mTglq",
        "outputId": "bca01dee-35cb-49bd-fa28-59befb6b1fcc"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "109486852"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n",
        "\n",
        "summary(model, input_size=(3, 1361))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "id": "aEsU7ao7K_3_",
        "outputId": "e765211b-4198-4ebb-d323-e0968833b7d2"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-9a3fd71230fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1361\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchsummary/torchsummary.py\u001b[0m in \u001b[0;36msummary\u001b[0;34m(model, input_size, batch_size, device)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;31m# make a forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;31m# print(x.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;31m# remove these hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1563\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1565\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1566\u001b[0m         )\n\u001b[1;32m   1567\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbw_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup_input_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1128\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1129\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_global_forward_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    970\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You have to specify either input_ids or inputs_embeds\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    971\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 972\u001b[0;31m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    973\u001b[0m         \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0minput_ids\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0minputs_embeds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    974\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "summary(model.cuda(), (df_new.shape))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "id": "dWVQi9qrKwKq",
        "outputId": "ac2b279d-f194-46ca-8b9c-b9dd0f194339"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-f543972d7b91>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdf_new\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchsummary/torchsummary.py\u001b[0m in \u001b[0;36msummary\u001b[0;34m(model, input_size, batch_size, device)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;31m# make a forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;31m# print(x.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;31m# remove these hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1563\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1565\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1566\u001b[0m         )\n\u001b[1;32m   1567\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbw_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup_input_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1128\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1129\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_global_forward_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    970\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You have to specify either input_ids or inputs_embeds\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    971\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 972\u001b[0;31m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    973\u001b[0m         \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0minput_ids\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0minputs_embeds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    974\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
          ]
        }
      ]
    }
  ]
}